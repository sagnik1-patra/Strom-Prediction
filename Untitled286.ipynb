{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9162893-4159-4d9c-96d8-2ac2b1efe020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "Hybrid Best Units: 130\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 12342720987136.0000 - mae: 82493.9688 - val_loss: 11331649536.0000 - val_mae: 13358.2041\n",
      "Epoch 2/50\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 12342712598528.0000 - mae: 82507.2891 - val_loss: 11330575360.0000 - val_mae: 13375.7080\n",
      "Epoch 3/50\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 12342698967040.0000 - mae: 82526.3984 - val_loss: 11329052672.0000 - val_mae: 13398.9551\n",
      "Epoch 4/50\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 12342677995520.0000 - mae: 82554.5781 - val_loss: 11326870528.0000 - val_mae: 13430.2295\n",
      "Epoch 5/50\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 12342644441088.0000 - mae: 82589.2031 - val_loss: 11324432384.0000 - val_mae: 13464.2451\n",
      "Epoch 6/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 12342609838080.0000 - mae: 82627.7109 - val_loss: 11321513984.0000 - val_mae: 13505.3408\n",
      "Epoch 7/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 12342564749312.0000 - mae: 82663.8047 - val_loss: 11317678080.0000 - val_mae: 13559.4814\n",
      "Epoch 8/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 12342513369088.0000 - mae: 82725.3047 - val_loss: 11313179648.0000 - val_mae: 13622.9180\n",
      "Epoch 9/50\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 12342465134592.0000 - mae: 82798.1328 - val_loss: 11308272640.0000 - val_mae: 13693.3037\n",
      "Epoch 10/50\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 12342384394240.0000 - mae: 82870.9375 - val_loss: 11302713344.0000 - val_mae: 13774.2041\n",
      "Epoch 11/50\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 12342302605312.0000 - mae: 82958.7656 - val_loss: 11296427008.0000 - val_mae: 13867.3125\n",
      "Epoch 12/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 12342198796288.0000 - mae: 83002.9219 - val_loss: 11291975680.0000 - val_mae: 13928.9355\n",
      "Epoch 13/50\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 12342081355776.0000 - mae: 83139.2578 - val_loss: 11280295936.0000 - val_mae: 14115.4473\n",
      "Epoch 14/50\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 12341959720960.0000 - mae: 83278.7109 - val_loss: 11272012800.0000 - val_mae: 14246.1094\n",
      "Epoch 15/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 12341825503232.0000 - mae: 83377.1953 - val_loss: 11262611456.0000 - val_mae: 14399.1562\n",
      "Epoch 16/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 12341672411136.0000 - mae: 83571.2109 - val_loss: 11252947968.0000 - val_mae: 14561.3779\n",
      "Epoch 17/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 12341515124736.0000 - mae: 83704.2031 - val_loss: 11242557440.0000 - val_mae: 14739.3076\n",
      "Epoch 18/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 12341342109696.0000 - mae: 83914.4844 - val_loss: 11232158720.0000 - val_mae: 14922.1895\n",
      "Epoch 19/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 12341166997504.0000 - mae: 84076.9766 - val_loss: 11221344256.0000 - val_mae: 15121.6426\n",
      "Epoch 20/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 12340966719488.0000 - mae: 84233.4453 - val_loss: 11209611264.0000 - val_mae: 15346.0938\n",
      "Epoch 21/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 12340749664256.0000 - mae: 84454.1875 - val_loss: 11197730816.0000 - val_mae: 15585.2178\n",
      "Epoch 22/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 12340526317568.0000 - mae: 84744.5078 - val_loss: 11186364416.0000 - val_mae: 15825.7861\n",
      "Epoch 23/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 12340299825152.0000 - mae: 84958.1484 - val_loss: 11174576128.0000 - val_mae: 16084.2783\n",
      "Epoch 24/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 12340060749824.0000 - mae: 85196.3203 - val_loss: 11162403840.0000 - val_mae: 16371.9287\n",
      "Epoch 25/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 12339795460096.0000 - mae: 85500.5938 - val_loss: 11150684160.0000 - val_mae: 16651.9316\n",
      "Epoch 26/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 12339530170368.0000 - mae: 85783.0391 - val_loss: 11139848192.0000 - val_mae: 16959.3301\n",
      "Epoch 27/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 12339251249152.0000 - mae: 86033.8828 - val_loss: 11128105984.0000 - val_mae: 17293.1992\n",
      "Epoch 28/50\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 12338946113536.0000 - mae: 86338.4844 - val_loss: 11116965888.0000 - val_mae: 17644.4844\n",
      "Epoch 29/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 12338632589312.0000 - mae: 86626.6641 - val_loss: 11107294208.0000 - val_mae: 17962.2051\n",
      "Epoch 30/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 12338309627904.0000 - mae: 87103.6328 - val_loss: 11097903104.0000 - val_mae: 18372.0547\n",
      "Epoch 31/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 12337989812224.0000 - mae: 87478.9219 - val_loss: 11088901120.0000 - val_mae: 18760.9668\n",
      "Epoch 32/50\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 12337664753664.0000 - mae: 87720.6875 - val_loss: 11080981504.0000 - val_mae: 19163.2578\n",
      "Epoch 33/50\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 12337310334976.0000 - mae: 88253.9062 - val_loss: 11074297856.0000 - val_mae: 19584.3652\n",
      "Epoch 34/50\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 12336965353472.0000 - mae: 88645.4297 - val_loss: 11069220864.0000 - val_mae: 20042.5977\n",
      "Epoch 35/50\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 12336586817536.0000 - mae: 88922.1797 - val_loss: 11065169920.0000 - val_mae: 20493.0215\n",
      "Epoch 36/50\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 12336200941568.0000 - mae: 89509.3984 - val_loss: 11062675456.0000 - val_mae: 20990.7461\n",
      "Epoch 37/50\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 12335795142656.0000 - mae: 89819.2812 - val_loss: 11061310464.0000 - val_mae: 21476.0547\n",
      "Epoch 38/50\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 12335380955136.0000 - mae: 90350.6719 - val_loss: 11063694336.0000 - val_mae: 22038.3262\n",
      "Epoch 39/50\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 12334972010496.0000 - mae: 91093.0547 - val_loss: 11066527744.0000 - val_mae: 22523.3984\n",
      "Epoch 40/50\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 12334571454464.0000 - mae: 91538.8828 - val_loss: 11071257600.0000 - val_mae: 23030.2441\n",
      "Epoch 41/50\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 12334166704128.0000 - mae: 91895.9141 - val_loss: 11078604800.0000 - val_mae: 23569.9004\n",
      "Epoch 42/50\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 12333717913600.0000 - mae: 92322.4375 - val_loss: 11088364544.0000 - val_mae: 24150.2812\n",
      "Epoch 43/50\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 12333268074496.0000 - mae: 93105.5312 - val_loss: 11100140544.0000 - val_mae: 24708.3301\n",
      "Epoch 44/50\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 12332815089664.0000 - mae: 93320.5312 - val_loss: 11105960960.0000 - val_mae: 25052.0137\n",
      "Epoch 45/50\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 12332343230464.0000 - mae: 94198.4062 - val_loss: 11132953600.0000 - val_mae: 25919.9316\n",
      "Epoch 46/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 12331873468416.0000 - mae: 94771.5938 - val_loss: 11153310720.0000 - val_mae: 26538.4473\n",
      "Epoch 47/50\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 12331414192128.0000 - mae: 95076.5078 - val_loss: 11163962368.0000 - val_mae: 26922.9180\n",
      "Epoch 48/50\n",
      "256/256 [==============================] - 0s 2ms/step - loss: 12330916118528.0000 - mae: 95892.3047 - val_loss: 11206296576.0000 - val_mae: 27875.4883\n",
      "Epoch 49/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 12330429579264.0000 - mae: 96552.8828 - val_loss: 11235531776.0000 - val_mae: 28528.3945\n",
      "Epoch 50/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 12329928359936.0000 - mae: 97321.1094 - val_loss: 11264696320.0000 - val_mae: 29139.0020\n",
      "80/80 [==============================] - 0s 1ms/step\n",
      "MSE: 413521113718.01544\n",
      "R²: 0.0034455328611505687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL FILES SAVED SUCCESSFULLY!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Dense\n",
    "from shapely.wkt import loads as wkt_loads\n",
    "import pickle\n",
    "import json\n",
    "import yaml\n",
    "import os\n",
    "import random\n",
    "\n",
    "# =========================================================\n",
    "# Paths\n",
    "# =========================================================\n",
    "base_path = r\"C:\\Users\\NXTWAVE\\Downloads\\Strom Prediction\"\n",
    "file_path = os.path.join(base_path, \"archive\", \"tornado_path.csv\")\n",
    "\n",
    "# =========================================================\n",
    "# Load Data\n",
    "# =========================================================\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Select only needed columns\n",
    "df = df[['state_name', 'property_loss', 'crop_loss',\n",
    "         'state_fips_code', 'storm_date', 'tornado_path_geom']]\n",
    "\n",
    "# =========================================================\n",
    "# Date → Year\n",
    "# =========================================================\n",
    "df['storm_date'] = pd.to_datetime(df['storm_date'], errors='coerce')\n",
    "df['year'] = df['storm_date'].dt.year\n",
    "\n",
    "# =========================================================\n",
    "# Geometry Feature Extraction (shapely)\n",
    "# =========================================================\n",
    "def extract_wkt_features(wkt_str):\n",
    "    try:\n",
    "        line = wkt_loads(wkt_str)\n",
    "        start = line.coords[0]\n",
    "        end = line.coords[-1]\n",
    "        return pd.Series({\n",
    "            'path_length': line.length,\n",
    "            'start_lon': start[0],\n",
    "            'start_lat': start[1],\n",
    "            'end_lon': end[0],\n",
    "            'end_lat': end[1],\n",
    "        })\n",
    "    except:\n",
    "        return pd.Series({\n",
    "            'path_length': np.nan,\n",
    "            'start_lon': np.nan,\n",
    "            'start_lat': np.nan,\n",
    "            'end_lon': np.nan,\n",
    "            'end_lat': np.nan,\n",
    "        })\n",
    "\n",
    "geo_features = df['tornado_path_geom'].apply(extract_wkt_features)\n",
    "df = pd.concat([df, geo_features], axis=1)\n",
    "\n",
    "# Drop unusable rows\n",
    "df = df.dropna()\n",
    "\n",
    "# =========================================================\n",
    "# Label Encoding (state_name)\n",
    "# =========================================================\n",
    "le = LabelEncoder()\n",
    "df['state_name'] = le.fit_transform(df['state_name'])\n",
    "\n",
    "# =========================================================\n",
    "# Prepare Data\n",
    "# =========================================================\n",
    "X = df.drop(['property_loss', 'storm_date', 'tornado_path_geom'], axis=1)\n",
    "y = df['property_loss']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# =========================================================\n",
    "# Define Model\n",
    "# =========================================================\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=X.shape[1]))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# =========================================================\n",
    "# AIS Optimization (simple immune clone selection)\n",
    "# =========================================================\n",
    "def AIS_optimize():\n",
    "    best_units = 64\n",
    "    best_loss = float('inf')\n",
    "    for _ in range(5):\n",
    "        units = random.choice([32, 64, 128])\n",
    "        model = Sequential([\n",
    "            Dense(units, activation='relu', input_dim=X.shape[1]),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
    "        loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_units = units\n",
    "    return best_units\n",
    "\n",
    "# =========================================================\n",
    "# PSO (particles search best neurons)\n",
    "# =========================================================\n",
    "def PSO_optimize(base_units):\n",
    "    particles = [max(16, base_units + random.randint(-20, 20)) for _ in range(5)]\n",
    "    best_units = particles[0]\n",
    "    best_loss = float('inf')\n",
    "    for p in particles:\n",
    "        model = Sequential([\n",
    "            Dense(p, activation='relu', input_dim=X.shape[1]),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
    "        loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_units = p\n",
    "    return best_units\n",
    "\n",
    "# =========================================================\n",
    "# Hybrid AIS + PSO Final Neurons\n",
    "# =========================================================\n",
    "ais_units = AIS_optimize()\n",
    "final_units = PSO_optimize(ais_units)\n",
    "\n",
    "print(\"Hybrid Best Units:\", final_units)\n",
    "\n",
    "# =========================================================\n",
    "# Train Final Hybrid Model\n",
    "# =========================================================\n",
    "hybrid_model = Sequential([\n",
    "    Dense(final_units, activation='relu', input_dim=X.shape[1]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "hybrid_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history = hybrid_model.fit(X_train, y_train, epochs=50, batch_size=32,\n",
    "                           validation_split=0.2, verbose=1)\n",
    "\n",
    "# =========================================================\n",
    "# Predictions\n",
    "# =========================================================\n",
    "pred = hybrid_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, pred)\n",
    "r2 = r2_score(y_test, pred)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R²:\", r2)\n",
    "\n",
    "# Save predictions\n",
    "pred_df = pd.DataFrame({'Actual': y_test.values, 'Predicted': pred.flatten()})\n",
    "pred_df.to_csv(os.path.join(base_path, \"hybrid_predictions.csv\"), index=False)\n",
    "\n",
    "# =========================================================\n",
    "# Save Model Files\n",
    "# =========================================================\n",
    "# 1. H5\n",
    "hybrid_model.save(os.path.join(base_path, \"hybrid_model.h5\"))\n",
    "\n",
    "# 2. JSON\n",
    "model_json = hybrid_model.to_json()\n",
    "with open(os.path.join(base_path, \"hybrid_model.json\"), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# 3. YAML\n",
    "with open(os.path.join(base_path, \"hybrid_model.yaml\"), \"w\") as yaml_file:\n",
    "    yaml.dump(json.loads(model_json), yaml_file)\n",
    "\n",
    "# 4. PKL\n",
    "with open(os.path.join(base_path, \"hybrid_model.pkl\"), \"wb\") as pkl_file:\n",
    "    pickle.dump(hybrid_model, pkl_file)\n",
    "\n",
    "# =========================================================\n",
    "# GRAPHS\n",
    "# =========================================================\n",
    "\n",
    "# 1. Heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pd.DataFrame(X).corr(), cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.savefig(os.path.join(base_path, \"hybrid_heatmap.png\"))\n",
    "plt.close()\n",
    "\n",
    "# 2. Accuracy Graph (Loss)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Hybrid Loss Curve\")\n",
    "plt.savefig(os.path.join(base_path, \"hybrid_accuracy.png\"))\n",
    "plt.close()\n",
    "\n",
    "# 3. Comparison Graph\n",
    "plt.scatter(y_test, pred)\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Hybrid Model Comparison\")\n",
    "plt.savefig(os.path.join(base_path, \"hybrid_comparison.png\"))\n",
    "plt.close()\n",
    "\n",
    "# 4. Prediction Graph\n",
    "plt.plot(pred[:100], label='Predicted')\n",
    "plt.plot(y_test.values[:100], label='Actual')\n",
    "plt.legend()\n",
    "plt.title(\"Hybrid Predictions\")\n",
    "plt.savefig(os.path.join(base_path, \"hybrid_prediction_graph.png\"))\n",
    "plt.close()\n",
    "\n",
    "# 5. Result Graph\n",
    "plt.bar(['MSE', 'R2'], [mse, r2])\n",
    "plt.title(\"Hybrid Results\")\n",
    "plt.savefig(os.path.join(base_path, \"hybrid_results.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(\"ALL FILES SAVED SUCCESSFULLY!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f9c167-a02e-4795-bc3a-d0d74b5532c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
