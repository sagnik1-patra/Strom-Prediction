{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "427c79d6-2f17-4c8c-b4f8-7ecba581cd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Dataset Loaded: (16000, 19)\n",
      "   storm_date storm_time time_zone_offset state_abbreviation state_name  \\\n",
      "0  2015-06-07   00:15:00           -06:00                 IA       Iowa   \n",
      "1  2015-11-11   14:00:00           -06:00                 IA       Iowa   \n",
      "2  2016-09-21   17:32:00           -06:00                 IA       Iowa   \n",
      "3  2008-05-01   17:59:00           -06:00                 IA       Iowa   \n",
      "4  2017-06-28   16:05:00           -06:00                 IA       Iowa   \n",
      "\n",
      "   state_fips_code magnitude  injured_count  fatality_count  property_loss  \\\n",
      "0               19         0              0               0          0.015   \n",
      "1               19         1              0               0          0.405   \n",
      "2               19         0              0               0       3000.000   \n",
      "3               19         2              0               0          0.510   \n",
      "4               19         2              0               0      75000.000   \n",
      "\n",
      "   crop_loss  yearly_tornado_count  start_lon  start_lat  end_long  end_lat  \\\n",
      "0      0.004                575683   -94.0213    42.0995  -93.9673  42.1081   \n",
      "1      0.000                602617   -94.5585    40.7157  -94.3545  40.9904   \n",
      "2   3000.000                614379   -92.7308    42.9155  -92.7105  42.9341   \n",
      "3      0.000                   553   -96.3000    43.1400  -96.4200  43.3000   \n",
      "4  10000.000                615497   -94.7771    40.5851  -94.6462  40.6043   \n",
      "\n",
      "   length   width                                  tornado_path_geom  \n",
      "0    2.83   120.0     LINESTRING(-94.0213 42.0995, -93.9673 42.1081)  \n",
      "1   21.80  1350.0  LINESTRING(-94.5585 40.7157, -94.4565 40.85305...  \n",
      "2    1.65   150.0     LINESTRING(-92.7308 42.9155, -92.7105 42.9341)  \n",
      "3   12.95  1200.0               LINESTRING(-96.3 43.14, -96.42 43.3)  \n",
      "4    6.99  3000.0     LINESTRING(-94.7771 40.5851, -94.6462 40.6043)  \n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "256/256 [==============================] - 2s 3ms/step - loss: 7907508224.0000 - mae: 2063.4512 - val_loss: 118835360.0000 - val_mae: 562.9297\n",
      "Epoch 2/50\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 7907287040.0000 - mae: 2074.2715 - val_loss: 118709520.0000 - val_mae: 585.8772\n",
      "Epoch 3/50\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 7906112512.0000 - mae: 2142.3914 - val_loss: 118152856.0000 - val_mae: 692.9964\n",
      "Epoch 4/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 7902823936.0000 - mae: 2323.9944 - val_loss: 117194528.0000 - val_mae: 829.7465\n",
      "Epoch 5/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 7897134080.0000 - mae: 2513.5046 - val_loss: 116356184.0000 - val_mae: 1175.7850\n",
      "Epoch 6/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 7892535808.0000 - mae: 2661.2917 - val_loss: 115983864.0000 - val_mae: 1418.3530\n",
      "Epoch 7/50\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 7888775680.0000 - mae: 3079.4116 - val_loss: 116225184.0000 - val_mae: 1636.3867\n",
      "Epoch 8/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 7885174272.0000 - mae: 3291.6396 - val_loss: 116396208.0000 - val_mae: 1761.2449\n",
      "Epoch 9/50\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 7880810496.0000 - mae: 3283.9265 - val_loss: 117632024.0000 - val_mae: 2042.5883\n",
      "Epoch 10/50\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 7878225920.0000 - mae: 3681.7896 - val_loss: 116310552.0000 - val_mae: 1893.4974\n",
      "Epoch 11/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 7873213952.0000 - mae: 3334.1741 - val_loss: 118039952.0000 - val_mae: 2223.3042\n",
      "Epoch 12/50\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 7875730944.0000 - mae: 3586.0076 - val_loss: 117981248.0000 - val_mae: 2243.9048\n",
      "Epoch 13/50\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 7867725824.0000 - mae: 3739.2014 - val_loss: 118738536.0000 - val_mae: 2311.3516\n",
      "Epoch 14/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 7872748032.0000 - mae: 3689.7219 - val_loss: 116465912.0000 - val_mae: 2206.8914\n",
      "Epoch 15/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 7865091072.0000 - mae: 3727.1533 - val_loss: 118311880.0000 - val_mae: 2410.6826\n",
      "Epoch 16/50\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 7861741568.0000 - mae: 3867.4961 - val_loss: 118337440.0000 - val_mae: 2353.0271\n",
      " 1/80 [..............................] - ETA: 7s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 1ms/step\n",
      "All files saved successfully in: C:\\Users\\NXTWAVE\\Downloads\\Strom Prediction\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Paths\n",
    "# -------------------------------------------------------------------\n",
    "base_path = r\"C:\\Users\\NXTWAVE\\Downloads\\Strom Prediction\"\n",
    "data_path = os.path.join(base_path, \"archive\", \"tornado_path.csv\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Load Dataset\n",
    "# -------------------------------------------------------------------\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(\"Dataset Loaded:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Basic Cleaning\n",
    "# -------------------------------------------------------------------\n",
    "df = df.dropna(subset=['crop_loss'])   # Target must not be empty\n",
    "\n",
    "# If tornado_path_geom contains geometry strings like LINESTRING\n",
    "if 'tornado_path_geom' in df.columns:\n",
    "    def extract_path_length(geom):\n",
    "        try:\n",
    "            # Example format: \"LINESTRING(lon lat, lon lat, ...)\"\n",
    "            coords = geom.replace(\"LINESTRING(\", \"\").replace(\")\", \"\")\n",
    "            coords = coords.split(\",\")\n",
    "            points = []\n",
    "            for c in coords:\n",
    "                lon, lat = map(float, c.strip().split(\" \"))\n",
    "                points.append((lon, lat))\n",
    "            points = np.array(points)\n",
    "\n",
    "            # Compute total path distance\n",
    "            d = np.sqrt(np.sum(np.diff(points, axis=0)**2, axis=1)).sum()\n",
    "            return d\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    df[\"path_length\"] = df[\"tornado_path_geom\"].apply(extract_path_length)\n",
    "else:\n",
    "    df[\"path_length\"] = 0\n",
    "\n",
    "# Drop unused\n",
    "df = df.dropna()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Feature Selection\n",
    "# -------------------------------------------------------------------\n",
    "features = ['property_loss', 'yearly_tornado_count', 'path_length']\n",
    "target = 'crop_loss'\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Train/Test Split\n",
    "# -------------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Scaling\n",
    "# -------------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save Scaler\n",
    "with open(os.path.join(base_path, \"scaler.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Build DNN Model\n",
    "# -------------------------------------------------------------------\n",
    "model = Sequential([\n",
    "    Dense(64, activation=\"relu\", input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(1)  # regression output\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Train Model\n",
    "# -------------------------------------------------------------------\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Save model (H5, JSON, YAML)\n",
    "# -------------------------------------------------------------------\n",
    "model.save(os.path.join(base_path, \"storm_model.h5\"))\n",
    "\n",
    "with open(os.path.join(base_path, \"storm_model.json\"), \"w\") as json_file:\n",
    "    json_file.write(model.to_json())\n",
    "\n",
    "with open(os.path.join(base_path, \"storm_model.yaml\"), \"w\") as yaml_file:\n",
    "    yaml.dump(json.loads(model.to_json()), yaml_file)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Predictions\n",
    "# -------------------------------------------------------------------\n",
    "preds = model.predict(X_test_scaled).flatten()\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"Actual\": y_test.values,\n",
    "    \"Predicted\": preds\n",
    "})\n",
    "results_df.to_csv(os.path.join(base_path, \"predictions.csv\"), index=False)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Evaluation\n",
    "# -------------------------------------------------------------------\n",
    "mse = mean_squared_error(y_test, preds)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, preds)\n",
    "\n",
    "summary = {\n",
    "    \"MSE\": float(mse),\n",
    "    \"RMSE\": float(rmse),\n",
    "    \"R2 Score\": float(r2)\n",
    "}\n",
    "\n",
    "with open(os.path.join(base_path, \"results.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=4)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Plots\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Loss Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Val Loss\")\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(base_path, \"loss_curve.png\"))\n",
    "plt.close()\n",
    "\n",
    "# 2. Prediction vs Actual\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(y_test, preds, alpha=0.6)\n",
    "plt.xlabel(\"Actual Crop Loss\")\n",
    "plt.ylabel(\"Predicted Crop Loss\")\n",
    "plt.title(\"Prediction vs Actual\")\n",
    "plt.savefig(os.path.join(base_path, \"prediction_vs_actual.png\"))\n",
    "plt.close()\n",
    "\n",
    "# 3. Correlation Heatmap\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(df[features + [target]].corr(), cmap='coolwarm', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.savefig(os.path.join(base_path, \"correlation_heatmap.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(\"All files saved successfully in:\", base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbe6ef6-e36e-45d8-b95c-db624e5ac1e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
